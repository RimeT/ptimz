{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Transfer Learning\n",
    "\n",
    "- [What is transfer learning](#What-is-transfer-learning)\n",
    "- [What we need to do in transfer learning](#What-we-need-to-do-in-transfer-learning)\n",
    "- [Same ResNet different input shape](#lets-transfer-a-resnet)\n",
    "- [Transfer ResNet to UNet](#resnet-to-resunet)\n",
    "- [Transfer UNet to ResNet](#transfer-resunet-to-resnet)\n",
    "\n",
    "### What is transfer learning\n",
    "Extract transferable representations from some source tasks and then adapt the gained representations to improve learning in related target tasks.\n",
    "\n",
    "### What we need to do in transfer learning\n",
    "- Load weights and bias from source model\n",
    "- Set weights and bias to target model\n",
    "- It's better the source model and the target model have some same structure/blocks\n",
    "\n",
    "### Let's transfer a ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from ptimz.model_zoo import ResNet\n",
    "from torch import nn\n",
    "from torch.nn.modules.batchnorm import _BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv.cnn import constant_init, normal_init\n",
    "def weight_init(module):\n",
    "    if isinstance(module, (nn.Conv3d, nn.Conv2d, nn.ConvTranspose3d, nn.ConvTranspose2d, nn.Linear)):\n",
    "        normal_init(module, mean=0.5, std=0.02)\n",
    "    elif isinstance(module, (_BatchNorm, nn.GroupNorm)):\n",
    "        constant_init(module, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source network input 2 channels, output 4 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "conv_cfg = dict(type='Conv3d')\n",
    "norm_cfg = dict(type='BN3d', requires_grad=True)\n",
    "source_net = ResNet(50,\n",
    "                in_channels=2,\n",
    "                deep_stem=True,\n",
    "                conv_cfg=conv_cfg,\n",
    "                norm_cfg=norm_cfg,\n",
    "                head_type='classification',\n",
    "                num_classes=4)\n",
    "\n",
    "# weight init\n",
    "source_net.apply(weight_init)\n",
    "\n",
    "source_net = source_net.cuda()\n",
    "\n",
    "input = torch.rand(1, 2, 64, 64, 64).cuda()\n",
    "with torch.no_grad():\n",
    "    output = source_net(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stem.0.conv.weight', 'stem.0.bn.weight', 'stem.0.bn.bias', 'stem.0.bn.running_mean', 'stem.0.bn.running_var', 'stem.0.bn.num_batches_tracked', 'stem.1.conv.weight', 'stem.1.bn.weight', 'stem.1.bn.bias', 'stem.1.bn.running_mean'] ['layer4.2.bn2.running_var', 'layer4.2.bn2.num_batches_tracked', 'layer4.2.conv3.weight', 'layer4.2.bn3.weight', 'layer4.2.bn3.bias', 'layer4.2.bn3.running_mean', 'layer4.2.bn3.running_var', 'layer4.2.bn3.num_batches_tracked', 'head_fc.weight', 'head_fc.bias']\n"
     ]
    }
   ],
   "source": [
    "src_state_dict = source_net.state_dict()\n",
    "param_keys = list(src_state_dict.keys())\n",
    "print(param_keys[:10], param_keys[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First layer weightout_channels x in_channels x kernel_size\n",
      "OUT CHANNESL x IN CHANNELS x KERNEL SIZE\n",
      "torch.Size([32, 2, 3, 3, 3]) tensor(0.5007, device='cuda:0')\n",
      "Last layer weight\n",
      "torch.Size([4, 2048]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print('First layer weightout_channels x in_channels x kernel_size')\n",
    "print('OUT CHANNESL x IN CHANNELS x KERNEL SIZE')\n",
    "print(src_state_dict['stem.0.conv.weight'].shape, torch.mean(src_state_dict['stem.0.conv.weight']))\n",
    "\n",
    "print('Last layer weight')\n",
    "print(src_state_dict['head_fc.weight'].shape, src_state_dict['head_fc.bias'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target network input 4 channels, output 8 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_cfg = dict(type='Conv3d')\n",
    "norm_cfg = dict(type='BN3d', requires_grad=True)\n",
    "target_net = ResNet(50,\n",
    "                in_channels=4,\n",
    "                deep_stem=True,\n",
    "                conv_cfg=conv_cfg,\n",
    "                norm_cfg=norm_cfg,\n",
    "                head_type='classification',\n",
    "                num_classes=8).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4, 3, 3, 3]) tensor(-3.7450e-05, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "target_state_dict = target_net.state_dict()\n",
    "print(target_state_dict['stem.0.conv.weight'].shape, torch.mean(target_state_dict['stem.0.conv.weight']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "def adapt_input_conv(in_chans, conv_weight):\n",
    "    # use linear interpolate on channels\n",
    "    conv_type = conv_weight.dtype\n",
    "    conv_weight = conv_weight.float()  # Some weights are in torch.half, ensure it's float for sum on CPU\n",
    "    weight_shape = list(conv_weight.shape)\n",
    "    conv_weight = conv_weight.permute(0, *list(range(2, len(weight_shape))), 1)\n",
    "    conv_weight = conv_weight.reshape(-1, 1, weight_shape[1])\n",
    "    conv_weight = F.interpolate(conv_weight, in_chans, mode='linear')\n",
    "    conv_weight = conv_weight.reshape(weight_shape[0], *weight_shape[2:], in_chans).permute(0, len(weight_shape) - 1,\n",
    "                                                                                            *list(range(1,\n",
    "                                                                                                        len(weight_shape) - 1)))\n",
    "    conv_weight = conv_weight.to(conv_type)\n",
    "    return conv_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4, 3, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['head_fc.weight', 'head_fc.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transfer the first layer\n",
    "src_state_dict['stem.0.conv.weight'] = adapt_input_conv(4, src_state_dict['stem.0.conv.weight'])\n",
    "print(src_state_dict['stem.0.conv.weight'].shape)\n",
    "\n",
    "# Drop the last layer\n",
    "del src_state_dict['head_fc.weight']\n",
    "del src_state_dict['head_fc.bias']\n",
    "\n",
    "# load parameters\n",
    "target_net.load_state_dict(src_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([32, 4, 3, 3, 3]) tensor(0.5007, device='cuda:0')\n",
      "resnet output torch.Size([1, 8]) tensor(-0.1146, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "target_state_dict = target_net.state_dict()\n",
    "print(\"weight\", target_state_dict['stem.0.conv.weight'].shape, torch.mean(target_state_dict['stem.0.conv.weight']))\n",
    "\n",
    "input = torch.rand(1, 4, 64, 64, 64).cuda()\n",
    "with torch.no_grad():\n",
    "    output = target_net(input)\n",
    "print(\"resnet output\", output.shape, torch.mean(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet to ResUNet\n",
    "\n",
    "Figure out the dataflow of backbone ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape torch.Size([1, 4, 64, 64, 64])\n",
      "stem out torch.Size([1, 64, 32, 32, 32])\n",
      "maxpool out torch.Size([1, 64, 16, 16, 16])\n",
      "layer1 torch.Size([1, 256, 16, 16, 16])\n",
      "layer2 torch.Size([1, 512, 8, 8, 8])\n",
      "layer3 torch.Size([1, 1024, 4, 4, 4])\n",
      "layer4 torch.Size([1, 2048, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"input shape\", input.shape)\n",
    "out = target_net.stem(input)\n",
    "print(\"stem out\", out.shape)\n",
    "out = target_net.maxpool(out)\n",
    "print(\"maxpool out\", out.shape)\n",
    "\n",
    "# reslayers\n",
    "for i in range(1, 5):\n",
    "    reslayer = getattr(target_net, f'layer{i}')\n",
    "    out = reslayer(out)\n",
    "    print(f\"layer{i} {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct UNet with ResNet backbone/encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([1, 4, 64, 64, 64])\n",
      "output torch.Size([1, 8, 64, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "from ptimz.model_zoo import UNet\n",
    "unet_encoders = [\n",
    "    target_net.stem,\n",
    "    nn.Sequential(target_net.maxpool, getattr(target_net, target_net.res_layers[0]))\n",
    "] + [getattr(target_net, x) for x in target_net.res_layers[1:]]\n",
    "extra = dict(num_channels=[64, 256, 512, 1024, 2048], nclasses=8)\n",
    "resunet = UNet(extra, unet_encoders, conv_cfg=conv_cfg, norm_cfg=norm_cfg).cuda()\n",
    "\n",
    "print(\"input\", input.shape)\n",
    "with torch.no_grad():\n",
    "    output = resunet(input)\n",
    "print(\"output\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['encoder_0.0.conv.weight', 'encoder_0.0.bn.weight', 'encoder_0.0.bn.bias', 'encoder_0.0.bn.running_mean', 'encoder_0.0.bn.running_var', 'encoder_0.0.bn.num_batches_tracked', 'encoder_0.1.conv.weight', 'encoder_0.1.bn.weight', 'encoder_0.1.bn.bias', 'encoder_0.1.bn.running_mean']\n",
      "tensor(0.5007, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "unet_state_dict = resunet.state_dict()\n",
    "unet_keys = list(unet_state_dict.keys())\n",
    "print(unet_keys[:10])\n",
    "print(torch.mean(unet_state_dict['encoder_0.0.conv.weight']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer ResUNet to ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init2(module):\n",
    "    if isinstance(module, (nn.Conv3d, nn.Conv2d, nn.ConvTranspose3d, nn.ConvTranspose2d, nn.Linear)):\n",
    "        normal_init(module, mean=-0.5, std=0.02)\n",
    "    elif isinstance(module, (_BatchNorm, nn.GroupNorm)):\n",
    "        constant_init(module, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unet first layer tensor(-0.4998, device='cuda:0')\n",
      "resnet first layer tensor(-0.4998, device='cuda:0')\n",
      "resnet output torch.Size([1, 8]) tensor(-0.0794, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "resunet.apply(weight_init2)\n",
    "unet_state_dict = resunet.state_dict()\n",
    "print(\"unet first layer\", torch.mean(unet_state_dict['encoder_0.0.conv.weight']))\n",
    "resnet_state_dict = target_net.state_dict()\n",
    "print(\"resnet first layer\", torch.mean(resnet_state_dict['stem.0.conv.weight']))\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = target_net(input)\n",
    "print(\"resnet output\", output.shape, torch.mean(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4, 3, 3, 3])\n",
      "torch.Size([32, 4, 3, 3, 3])\n",
      "True True\n"
     ]
    }
   ],
   "source": [
    "unet_fl = resunet.encoder_0[0].conv.weight\n",
    "print(unet_fl.shape)\n",
    "resnet_fl = target_net.stem[0].conv.weight\n",
    "print(resnet_fl.shape)\n",
    "print(unet_fl is resnet_fl, id(unet_fl) == id(resnet_fl))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}